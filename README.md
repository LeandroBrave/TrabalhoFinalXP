# TrabalhoFinalXP# üìä Kafka Pipeline - Tesouro IPCA e Tesouro PRE

Este reposit√≥rio cont√©m um pipeline de dados em tempo real que extrai dados do Tesouro IPCA e Tesouro PRE de um banco PostgreSQL, envia para o Apache Kafka e distribui para destinos configur√°veis via Kafka Connect.

---

### üë§ Autor

**Leandro Valente** ‚Äì [@LeandroBrave](https://github.com/LeandroBrave)


## ‚öôÔ∏è Tecnologias Utilizadas

- **Apache Kafka** ‚Äì Sistema de mensagens distribu√≠das.
- **Kafka Connect** ‚Äì Framework para integra√ß√£o com sistemas externos.
- **PostgreSQL** ‚Äì Banco de dados relacional utilizado como fonte e/ou destino.
- **Kafka JDBC Connector** ‚Äì Conector para integra√ß√£o JDBC com PostgreSQL.
- **Docker e Docker Compose** ‚Äì Containeriza√ß√£o dos servi√ßos.

---

## üöß Melhorias Futuras

- Criar orquestra√ß√£o, provavelmente com airflow
- Fazer deploy automatico de arquivos de configura√ß√£o quando novos consumidores ou produtores entrarem na pipeline
- suporte a mais plataformas alem do posgres e s3


## üöÄ Como Rodar o Projeto

### Passo 1: Clonar o reposit√≥rio

    Clone este reposit√≥rio para sua m√°quina local

### Passo 2: Conta AWS

    Crie uma conta amazon se n√£o tiver uma
    Crie um usuario atrav√©s do AWS IAM
    Crie um arquivo no projeto nomeado: .env_kafka_connect
    Escreva no .env_kafka_connect o conteudo abaixo:
        
        AWS_ACCESS_KEY_ID=[PREENCHA COM O VALOR DO SEU USUARIO AWS]
        AWS_SECRET_ACCESS_KEY=[PREENCHA COM O VALOR DO SEU USUARIO AWS]

    Acesse o S3 e crie 2 buckets com a seguinte estrutura de pastas:
    Buckets:
        dados-pre
        dados-ipca
    Em cada bucket crie a estrutura de pastas:
        raw_data/kafka

### Passo 3: Docker
    Crie a imagem kafka:
    **Comando:**

        cd connect/custom-kafka-connectors-image
        docker buildx build . -t connect-custom:1.0.0

    Suba todos os servi√ßos contidos no arquivo docker-compose:

    **Comando:** docker-compose up

### Passo 4: Setup Postgres

**crie um arquivo .env com o conteudo abaixo:**

    PG_URL=jdbc:postgresql://localhost:5432/postgres
    PG_USER=postgres
    PG_PASSWORD=postgres
    PG_DB=postgres
    PG_HOST=localhost
    PG_PORT=5432
    PG_DRIVER=org.postgresql.Driver

### Passo 5: Camada Bronze
    Execute o arquivo:
     Na pasta ingestao:
             importar.ipynb

### Passo 6: Camadas Silver e Gold
    Crie o schema Silver e o schema Gold no postgres
    Rode os respectivos arquivos SQL em cada schema
    Execute o codigo main.py

### Passo 7: Kafka
    Entre na maquina do broker que o docker subiu:
        **Comando:** docker exec -it broker bash
    
    Crie 2 topicos do kafka:
        **Comandos:** 
    
            kafka-topics --create --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --topic postgres-dadostesouroipca
            kafka-topics --create --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --topic postgres-dadostesouropre
    
    Cheque se os topicos foram criados:
        **Comando:** kafka-topics --bootstrap-server localhost:9092 --list
    
    Digite exit para voltar ao seu terminal local

### Passo 8: Criar os conectores Source
    Os conectores do tipo source s√£o os produtores. Eles jogam dados dentro do topico kafka.
    Criaremos 2 deles:
    **Comandos:**

        curl -X POST -H "Content-Type: application/json" --data @connect_jdbc_postgres_ipca.config http://localhost:8083/connectors
        curl -X POST -H "Content-Type: application/json" --data @connect_jdbc_postgres_pre.config localhost:8083/connectors

     Retorne para a maquina do broker novamente
     **Comando:** 
         docker exec -it broker bash

    Execute o consumer para verificar a atividade do dado que √© produzido
    **Comando:**

        kafka-console-consumer --bootstrap-server localhost:9092 --topic postgres-dadostesouroipca --from-beginning
        kafka-console-consumer --bootstrap-server localhost:9092 --topic postgres-dadostesouropre --from-beginning

### Passo 9: Criar os conectores sink
    Os conectores do tipo sink s√£o os consumidores. Eles leem dados do topico kafka e escrevem em algum lugar.
    Criaremos 2 deles:
    **Comandos:**

        curl -X POST -H "Content-Type: application/json" --data @connect_s3_sink_ipca.config http://localhost:8083/connectors
        curl -X POST -H "Content-Type: application/json" --data @connect_s3_sink_pre.config http://localhost:8083/connectors

### Passo 10: Cheque seu AWS s3
    Ap√≥s configura√ß√£o do sink os dados confira se os dados chegaram nos buckets que foram criados no passo 2
